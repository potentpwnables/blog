---
title: Piping API Data To Google Big Query Using Python - Part I
author: ''
date: '2020-06-29'
slug: piping-api-to-gbq-python-part-i
categories: []
tags:
  - python
  - api
  - bigquery
type: ''
subtitle: ''
image: ''
---

This is the first of a two-part blog post:
* [Building the API](https://tibblesnbits.com/2020/06/29/piping-api-to-gbq-python-part-i)
* [Piping it to Google BigQuery](https://tibblesnbits.com/2020/06/29/piping-api-to-gbq-python-part-ii)

Welcome to the inaugural post for my blog! Well... sort of inaugural. I've created and destroyed this blog a hundred times, but that's neither here nor there. What has brought me back from the ashes to write is a task that a good friend of mine asked me to help with. It was something that had a lot of elements to it, and it nerd sniped me pretty good. Basically, the task was to perform an initial dump from the Qualys API so that he had all of the historical data, and then set up a cron job that would grab any new data every three hours. That data was then to be fed into Google BigQuery (GBQ) so that dashboards could be made, so it had to relatively fast and reliable. The problem was that the historical data was approximately 50GB worth of XML, so it couldn't all be held in memory and processed. This got me thinking about various solutions, and I think I've got something that will work quite nicely. 

Before we begin, it's worth pointing out that this post will not be a thorough tutorial on every step. The onus of things like setting up VMs, working with certain files, and setting up a GBQ account will be put on you.

So let's lay out what we would need to do if we were my friend:

1. Connect to the API and start pulling the data with Python  
2. Process the data inside of Python as it streamed  
3. Push that data to Google's BigQuery  
4. Repeat steps 1-3 for any new data that comes in after our initial pull

But here's the rub: I don't have access to the API that he's using, so that complicates my ability to create a solution tailored specifically to his need. But it also creates an opportunity to learn a little bit more about APIs than I knew before by building one from scratch. And that also means we'll need some data of our own. Once we have that, we can build our client-side processer and push the results to BigQuery. So here's what our steps will look like:

1. Set up two VMs to mimic having a client and server  
2. Download the flows.txt.gz file from [LANL](https://csr.lanl.gov/data/cyber1/)  
3. Clean the data  
4. Import the data into a sqlite database  
5. Create the API using Flask  
6. Grab a subset of the data  
7. Process it  
8. Send it to GBQ  
9. Set up a script to iteratively grab newer chunks of the data  
10. Profit

#### Set up our VMs
First and foremost, I want to say that I'm using two VMs because I didn't want to clutter my host with the stuff we'd be creating on the client machine. It's worth pointing out that you could absolutely create a single VM to serve as the server, and connect to the API from your host. Additionally, I'm not going to go into any level of detail here as there are TONS of blogs, videos, etc on how to set up a virtual machine.

But what I will say is that both machines are running Ubuntu 20.04 via VirtualBox using a Bridged network adapter. I'm not an expert, so it might be possible to achieve this using a NAT network adapter, but in order to get the two machines to be able to talk to each other I had to use Bridged. In order to ensure that your client VM can talk to the server VM, you should run `ip addr show` on both machines, and then have each machine ping the other, just to make sure they can see each other. 

For specs, I've set it up so that the VM acting as the server, which will house the sqlite database and the API, is using a single CPU and 2 GB of RAM. The VM that I'm using to actually pull the data from the API is using 2 CPU and 4 GB of RAM. Each machine has a cap of 100GB of storage. The purpose for doing this was to show that this method of handling the data as it streams in allows us to work with very large data using minimal hardware.

#### Download and clean the data
Getting the data is a bit of a hassle because LANL changed their website to now ask you for your email and what you're going to use the data for. You don't have to provide real answers, but I haven't seen them send any spam email to me, and seeing that people are using the data they provide likely incentives them to continue to provide data. So I'd encourage you to at least provide a real reason (e.g. "Creating an API from scratch and using this as the database"). Anyway, once you do that, you can just click the file and the download will start. I opted for the flows.txt.gz file because it was pretty small and we don't need a lot of data. The original ask involved about 50GB of data, that included all of the overhead of XML, and as we'll see later on, that small file ends up being a lot of data once we add in that overhead.

Once we have the data, we'll need to decompress it and then do a couple of preprocessing things. Running `gunzip flows.txt.gz` will give us the files that we need, so now we can begin preprocessing the data. The first thing we want to do is ensure that every row has only the data that we want. What this means is that if a data ingestion tool were to try to import this file, which is a CSV, using commas as delimiters, would it find the exact number of columns that it expected on each row? We can ensure that it does by check the number of columns ourself first. Since this is just test data, we don't have to worry about what to do if some of the records are incorrect, but in the real world that's definitely something you'd want to consider. For us, we can run the following command:

```
cat flows.txt | awk -F , 'NF == 9{print}{}' >> flows.csv
```

What this is doing is piping the file into awk and telling awk to use the comma as a delimeter (`-F ,`) and then check if there are 9 fields (`NF == 9`) on that line. If there are, then print the line (`{print}`), otherwise do nothing (`{}`). The output is then piped into flows.csv. The end result is a CSV file that is guaranteed to have the exact number of fields expected on each line, which will prevent any ingest errors when we load it into sqlite.

The next step is add the row number to each line. The reason for this is that the flows data does not have a single column that is unique to every row, and Flask will complain if you try to feed it a table that doesn't have a primary key defined in the table. This is kind of annoying because sqlite would have given us a primary key for free via `ROWID`, but Flask forces us to set our own. To do this, we'll again use awk.

```
mv flows.csv flows.txt
cat flows.txt | awk '{printf "%s,%s\n", NR, $0}' >> flows.csv
```

Again, we're piping the file into awk - and yes, I know you can just feed the file in; get off my back - and then using `printf` to format the line before printing. The `"%s,%s\n"` is the format of the line, which accepts two variables, designated by the `%s`, separated by a comma and followed by a new line. `NR` is the row number, and is our first variable, and `$0` is the line of data, and is our second variable. Once this is finished running, if you run `head -n 3 flows.csv`, you should see something that looks like this:

```
1,1,0,C1065,389,C3799,N104561,6,10,5323
2,1,0,C1423,N1136,C1707,N1,6,5,847
3,1,0,C1423,N1142,C1707,N1,6,5,847
```

#### Import the data into sqlite
Assuming that went well, we're ready to import the data into sqlite. To do this, we'll first need to install sqlite3 on our machine by running `sudo apt install sqlite3`. Once that's installed, run `sqlite3 flows.db` and that'll drop you into sqlite inside of the newly created flows.db file (it creates the file in the current working directory if it doesn't already exist). Your terminal should look something like this:

```sql
SQLite Version 3.31.1 2020-01-27 19:55:54
Enter ".help" for usage hints.
sqlite> 
```

Our next step is to create the table that we're going to store our data in. If you've worked with SQL before, this is probably pretty familiar to you, but if you haven't, I'd recommend taking some time to read a couple blog posts about it. But even if you haven't, all of the following will hopefully still be pretty readable for you. At the prompt, type the following snippet of code.

```sql
CREATE TABLE flows(
    id INTEGER PRIMARY KEY, 
    time INTEGER NOT NULL, 
    duration INTEGER NOT NULL, 
    src_comp TEXT NOT NULL, 
    src_port TEXT NOT NULL, 
    dst_comp TEXT NOT NULL, 
    dst_port TEXT NOT NULL, 
    protocol INTEGER NOT NULL, 
    packet_count INTEGER NOT NULL, 
    byte_count NOT NULL)
    WITHOUT ROWID;
```

If you don't get any errors, you should be able to run `.tables` and have it return "flows" to you. You should also be able to run `.schema` and have it return back what you just typed in. If that all works, we can now import our CSV that we created. One thing you may have noticed is that our CSV doesn't have a header row on it, saying what the columns are. This is by design since sqlite already knows what the columns are. It expects that the data being read in is in the order in which you specified the columns in your `CREATE TABLE` statment. So to do this, we'll run two commands. The first tells sqlite that we're about to deal with some CSV data, and the second imports the data.

```
.mode csv
.import /absolute/path/to/flows.csv flows
```

The first argument to `.import` is the path to the file you want to import, and the second argument is the name of the table you want to store it in. This is going to take a minute to run, so feel free to grab some coffee or something.

#### Create the API using Flask
Finally, we get to the fun part! Again, this is not a tutorial on all things, so I won't be diving into the granular details of Flask, but I'll be explaining the key things as we go. To get started, we need to install Flask on to our server VM (this is the one with 1 CPU and 2GB of RAM). To do this, we'll first need to install pip, so run the following commands on the VM that will be your server.

```
sudo app install python3-pip
pip3 install flask
```

To make sure this works, enter Python from the terminal and run `from flask import Flask`. If that works, you're good to go. So let's dive into the code. Create a file called "app.py". That's going to hold all of the code we need for our API. We'll start by just enumerating the modules we're importing.

```python
from flask import Flask, jsonify, request, Response
import sqlite3
import os
import json
```

This should be relatively straight-forward. The next thing we're going to do is initialize a couple of variables upfront.

```python
basedir = os.path.abspath(os.path.dirname(__file__))
app = Flask(__name__)
```

The first line isn't really necessary, but it allows us to ensure that our base directory is always the directy in which app.py lives, which is useful for connecting to the database, and allows us to call app.py from anywhere. The second line initializes our Flask app using the `__name__` variable, which in our situation will just be equal to "__main__". You can read more about Flask [here](https://flask.palletsprojects.com/en/1.1x/api/). We'll finish with the skeleton of the file (the base code needed to run the Flask app) by telling the script to run the app if app.py is called directly.

```python
if __name__ == "__main__":
    app.run(debug=True, host="0.0.0.0")
```

Setting debug equal to True allows us to keep the app running while we change things in app.py without having to stop the app and restart it. Setting host equal to "0.0.0.0" allows the app to listen for connections from more than just the localhost, which is crucial for being able to connect to the API from our client VM. 

Now that we have our skeleton, let's add some functionality so we can check that it's actually working. Add the following code just above the `if __name__ ==` line.

```python
@app.route("/api/v1/stream/flows.json", methods=["GET"])
def get():
    return Response(
        json.dumps({"msg": "Hello World!"}), 
        mimetype="application/json"
    )
```

Save the file and exit and then run `python3 app.py`. You should see something like the following:

```bash
* Serving Flask app "app" (lazy loading)
* Environment: production
  Warning: This is a development server. Do not use it in a production deployment.
  Use a production WSGI server instead.
* Debug mode: on
* Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)
* Restarting with stat
* Debugger is active!
* Debugger PIN: ###-###-###
```

If you see that, head over to your client VM and open a browser (or just use the browser on your host) and go to `http://<private IP of your server>:5000/api/v1/stream/flows.json`, where the private IP address of your server should be something like 192.168.1.*. You should be created with JSON data that says "Hello world". But that's not a very useful API, so we'll add the real functionality now. The code we just added is what's called a route, and it tells Flask two things:

1. What end points should users be able to hit, and  
2. What Flask should do when users hit a specific end point.

So as you may have deduced from the code and the URL that you navigated to, "/api/v1/stream/flows.json" is the initial endpoint that we're creating for this API. The second parameter that was passed to `@app.route` is the methods that the end point should accept, with GET and POST being the two most common. The rest of the code that we added is what the endpoint should do when a user hits that endpoint with the method we specified. In our quick example, it returned a simple JSON-like string (that's what `json.dumps` does) to the client. We're going to change this code to now connect to our database, run a query, and stream the results back to the client. 

In your file, replace this:

```python
def test():
    return Response(
        json.dumps({"msg": "Hello World!"}), 
        mimetype="application/json"
    )
```

with this:

```python
def query_flows():
  conn = sqlite3.connect(basedir + "/flows.db")
  c = conn.cursor()
  q = "SELECT * FROM flows LIMIT 10"
  c.execute(q)
  return Response(stream(conn, c), mimetype="application/json")
```

The first line of our new code is us connecting to our sqlite database. This is a persistent connection and will stay open until we close it, which you should be mindful of because too many open connections on a database is problematic. The second line creates a cursor inside of our database, which acts as a pointer. The third line is our query. We're going to grab all of the variables, but only the first 10 records for now. Best to test small before going big. The fourth line "executes" the query. I put execute in quotes because while it does execute the query, it doesn't actually return any of the data, which is advantageous for us. The last line returns the output of a currently undefined function called stream as a `Response` object. To really take advantage of this, we need to create a generator that will yield each record from our database to `Response` so it can send it to the client and get ready for the next record. To do this, we'll create a new function called stream. Put the following code above your `@app.route` line.

```python
def stream(conn, records):
  try:
    prev_record = next(records)
  except StopIteration:
    yield '{}'
    raise StopIteration
  
  for record in records:
    yield json.dumps(format_record(prev_record)) + "\n"
    prev_record = record
  
  conn.close()
  yield json.dumps(format_record(prev_record))
```

What we've built here is a lagged generator. Because we're appending a delimiter to each JSON record - here we're using a new line character - we need to make sure we don't append it to the last record that gets returned. Otherwise, we'd be sending malformed JSON to the user. To overcome this, we'll jump start the iteration by calling `next` on the iterator to get the first result before we even start our loop. What this means is that once we begin our loop via `for record in records`, the first value that `record` gets is actually the second value in `records`, not the first. That value is stored in `prev_record`. So we jump in at the second record, yield the first record, and then set `prev_record` to the current record. Assume there are five records in the table that we query. If you follow this through, you'll see that once we get to the fifth and final record, we'll be returning the fourth record with our delimiter attached, which means we still need to return our fifth record. We do that with our final `yield` call in the function, just after we close our connection to the database. If you're not familiar with the `yield` keyword, I would highly encourage looking it up, as it has some very powerful properties.

But let's look at this function with a little more detail and walk through it line by line. The first thing we notice is that the function takes two parameters: conn and records. If we look at the line in our code where we call `stream`, we'll see that we're passing in `conn`, which is our database connection, as the first parameter, and `c`, which is our cursor that is holding the query we executed, as the second parameter. I named it records in the stream function because that's what it represents. `c` is an iterable that on each call will return the next record from the result set that is generated by the query. Once all results have been returned, `c` has been exhausted. So looping over `c` is really looping over the records. The next thing we do is utilize a try/except structure to capture any events in which our query returns no results. In this case, we'll return an empty JSON-like string. Assuming there are results, we loop over them returning the data as I described above until we've exhausted the results. Once all but the last record has been returned to the client, we close the database connection and then return that final record.

With this functionality in place, we're left with just one more function to define, which is the `format_record` function that we utilize in `stream`. This function does nothing other than convert the record that is returned from a tuple to a dict so that it can be passed as JSON to the client. It's defined below and should be self explantory.

```python
def format_record(record):
  return {
    "id": record[0],
    "time": record[1],
    "duration": record[2],
    "src_comp": record[3],
    "src_port": record[4],
    "dst_comp": record[5],
    "dst_port": record[6],
    "protocol": record[7],
    "packet_count": record[8],
    "byte_count": record[9]
  }
```

And now, with that, you should be able to refresh the page in your browser that's calling the API and see the first 10 records of the flows table returned to you as JSON. Congratulations! You just built an API. Well, sort of. At this point we've really just served a JSON file from an endpoint, whereas an actual API would allow you to pass parameters to the endpoint that customizes what data you get back. In the example of what my friend needs, we need the ability to specify a "scanned_after" parameter to avoid grabbing data that we've already grabbed. The default behavior of the API should be that if this parameter is not passed, it returns all data. Lastly, we'll also introduce a "scanned_before" parameter. This will be useful for defining a rolling window for which we want to return data. 

Implementing this is thankfully relatively easy thanks to the `request` attribute from Flask. `request` has a property called "args", which will hold the parameters we pass to the URL in a dictionary that we can access. To see how this works, let's create a temporary endpoint that will just return our parameters back to us. Add the following snippet of code just above your `if __name__` line. 

```python
@app.route("/api/v1/test", methods=["GET"])
def get_parameters():
  return jsonify(request.args)
```

Now, if you navigate to that endpoint in your browser via `http://<private IP of your server>:5000/api/v1/test?x=1&y=2` you should be presented with JSON data showing `{"x": 1, "y": 2}`. Hopefully this demonstrates to you how we can access these parameters and use them in our API call. Feel free to play around with some examples of this, and once you feel you understand what it's doing, delete that entire snippet of code from your app.py file.

Because the API parameters are automatically collected in a dictionary, we can check that dictionary for the presence of "scanned_after" or "scanned_before" and embed those into our query to avoid returning all of the results. To do this, we're going to add a couple of lines to our `query_flows` function. We'll add two lines to try to grab the two parameters we're interested in, and then we'll change our query to implement these parameters. Replace your `query_flows` function with the following code:

```python
def query_flows():
  conn = sqlite3.connect(basedir + "/flows.db")
  c = conn.cursor()
  start = request.args.get("scanned_after", 0)
  end = request.args.get("scanned_before", 5011300)
  q = "SELECT * FROM flows WHERE time > ? AND time <= ?"
  c.execute(q, (start, end))
  return Response(stream(conn, c), mimetype="application/json")
```

The two lines that we added utilize `request.args.get`, which grabs the `args` property from `request`, which we just saw was a dictionary, which has a method called `get` that allows us to try to access a key, but if that key doesn't exist it will return a default value. For `start`, we've set the default value to 0 since if it's not present we don't want to exclude any records, and similarly for `end` we set the default to a value that is larger than any value in the data. There are multiple ways to handle this, but this was the simplest approach for the data we're working with. 

We also modified our query to include `WHERE time > ? AND time <= ?`, which time bounds the data that is returned to the user. The `?` is `sqlite3`'s way of using placeholders to pass in variables to your query in a safe way. This allows the sanitization of the inputs to be handled by the module instead of by us, which is very nice. In order to pass in our variables, we pass them as a tuple to `c.execute()` in the order in which we want them placed into the query. Notice that we've also removed the `LIMIT` on our query since we're pretty close to being ready for production.

With this functionality in place, you should be able to navigate to `http://<private IP of your server>:5000/api/v1/stream/flows.json?scanned_after=86400&scanned_before=86401` to get a single second of activity from the data. Note that `time` in our data is the number of seconds from some point in time that was determined by LANL. We don't actually have dates, and only know that the data covers 58 days of activity. Thus, the parameters we just passed would represent the first second of activity on the second day.

*IMPORTANT NOTE:* You will most likely get a SyntaxError when you run this request. This is because we're technically returning invalid JSON to the user because it's not contained in a single container, either `[...]` or `{results: [...]}`. But if you click the "Raw Data" tab at the top of the window (at least on Firefox that's how you do it, not sure how to do it on Chrome) you'll see that the data is in fact being returned. In a real app, we'd ensure that we handled that.

The last thing we're going to do is make it so that our API can return both JSON and XML. We started with JSON because it's easier to work with, aligns with the tutorials you can find online, and is objectively the better data format. But since this was all inspired by the need to ingest XML, we'll want that format for the next part of this blog series. To do this, we'll add another line to `query_flows` to check the API call for a `format` parameter, which we'll default to JSON, and then we'll modify `format_records` to return JSON or XML on demand.

Change the endpoint to no longer specify the format:

```python
@app.route("/api/v1/stream/flows", methods=["GET"])
```

Add the following lines to `query_flows`:

```python
fmt = request.args.get("format", "json")
content_type = f"application/{fmt}"
```

Change the return statement in `query_flows` to be:

```python
return Response(stream(conn, c, fmt), mimetype=content_type)
```

define `stream` as follows:

```python
def stream(conn, records, fmt):
  try:
    prev_record = next(records)
  except StopIteration:
    if fmt == "json":
      yield "{}"
    else:
      yield "<?xml verion='1.0' encoding='UTF-8'?><records/>"
    raise StopIteration
    
  if fmt == "xml":
    yield "<?xml version='1.0' encoding='UTF-8'?><records>"
  for record in records:
    yield format_record(prev_record, fmt) + "\n"
    prev_record = record
  
  conn.close()
  yield format_record(prev_record, fmt)
  if fmt == "xml":
    yield "</records>"
```

Pass the foramt to the `format_record` function (make sure to change this line in both locations):

```python
yield json.dumps(format_record(prev_record, fmt))
```

And lastly, define `format_record` as follows:

```python
def format_record(record, fmt):
  if fmt == "json":
    return {
      "id": record[0],
      "time": record[1],
      "duration": record[2],
      "src_comp": record[3],
      "src_port": record[4],
      "dst_comp": record[5],
      "dst_port": record[6],
      "protocol": record[7],
      "packet_count": record[8],
      "byte_count": record[9],
    }
  else:
    s = """
    <record>
      <id>%d</id>
      <time>%d</time>
      <duration>%d</duration>
      <src_comp>%s</src_comp>
      <src_port>%s</src_port>
      <dst_comp>%s</dst_comp>
      <dst_port>%s</dst_port>
      <protocol>%d</protocol>
      <packet_count>%d</packet_count>
      <byte_count>%d</byte_count>
    </record>
    """ % record
    return s.strip().replace("\n", "").replace(" ", "")
```

If you've set this up correctly - the full code is pasted below for you to compare to - you can now naivate to `http://<private IP of your server>:5000/api/v1/stream/flows?scanned_after=86400&scanned_before=86401&format=xml` and see XML results returned back to you. And if that's the case, then now you can celebrate because you've officially created an API! Be sure to check out the next part of the series where we pull data from this API, process it, and send it to GBQ.

#### Full Code

```python
from flask import Flask, request, Response
import sqlite3
import os
import json

basedir = os.path.abspath(os.path.dirname(__file__))
app = Flask(__name__)

def format_record(record, fmt):
  if fmt == "json":
    return {
      "id": record[0],
      "time": record[1],
      "duration": record[2],
      "src_comp": record[3],
      "src_port": record[4],
      "dst_comp": record[5],
      "dst_port": record[6],
      "protocol": record[7],
      "packet_count": record[8],
      "byte_count": record[9],
    }
  else:
    s = """
    <record>
      <id>%d</id>
      <time>%d</time>
      <duration>%d</duration>
      <src_comp>%s</src_comp>
      <src_port>%s</src_port>
      <dst_comp>%s</dst_comp>
      <dst_port>%s</dst_port>
      <protocol>%d</protocol>
      <packet_count>%d</packet_count>
      <byte_count>%d</byte_count>
    </record>
    """ % record
    return s.strip().replace("\n", "").replace(" ", "")
    
def stream(conn, records, fmt):
  try:
    prev_record = next(records)
  except StopIteration:
    if fmt == "json":
      yield "{}"
    else:
      yield "<?xml verion='1.0' encoding='UTF-8'?><records/>"
    raise StopIteration
    
  if fmt == "xml":
    yield "<?xml version='1.0' encoding='UTF-8'?><records>"
  for record in records:
    yield format_record(prev_record, fmt) + "\n"
    prev_record = record
  
  conn.close()
  yield format_record(prev_record, fmt)
  if fmt == "xml":
    yield "</records>"

@app.route("/api/v1/stream/flows", methods=["GET"])
def query_flows():
  conn = sqlite3.connect(basedir + "/flows.db")
  c = conn.cursor()
  start = request.args.get("scaned_after", 0)
  end = request.args.get("scanned_before", 5011300)
  fmt = request.args.get("format", "json")
  content_type = f"application/{fmt}"
  q = "SELECT * FROM flows WHERE time > ? AND time <= ?"
  c.execute(q, (start, end))
  return Response(stream(conn, c, fmt), mimetype=content_type)
  
if __name__ == "__main__":
  app.run(debug=True, host="0.0.0.0")
```