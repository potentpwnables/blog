---
title: Piping API Data To Google Big Query Using Python - Part I
author: ''
date: '2020-06-29'
slug: piping-api-to-gbq-python-part-i
categories: []
tags:
  - python
  - api
  - bigquery
type: ''
subtitle: ''
image: ''
---

This is the first of a two-part blog post:
* [Building the API](https://tibblesnbits.com/2020/06/29/piping-api-to-gbq-python-part-i)
* [Piping it to Google BigQuery](https://tibblesnbits.com/2020/06/29/piping-api-to-gbq-python-part-ii)

Welcome to the inaugural post for my blog! Well... sort of inaugural. I've created and destroyed this blog a hundred times, but that's neither here nor there. What has brought me back from the ashes to write is a task that a good friend of mine asked me to help with. It was something that had a lot of elements to it, and it nerd sniped me pretty good. Basically, the task was to perform an initial dump from the Qualys API so that he had all of the historical data, and then set up a cron job that would grab any new data every three hours. That data was then to be fed into Google BigQuery (GBQ) so that dashboards could be made, so it had to relatively fast and reliable. The problem was that the historical data was approximately 50GB worth of XML, so it couldn't all be held in memory and processed. This got me thinking about various solutions, and I think I've got something that will work quite nicely. 

Before we begin, it's worth pointing out that this post will not be a thorough tutorial on every step. The onus of things like setting up VMs, working with certain files, and setting up a GBQ account will be put on you.

So let's lay out what we would need to do if we were my friend:

1. Connect to the API and start pulling the data with Python  
2. Process the data inside of Python as it streamed  
3. Push that data to Google's BigQuery  
4. Repeat steps 1-3 for any new data that comes in after our initial pull

But here's the rub: I don't have access to the API that he's using, so that complicates my ability to create a solution tailored specifically to his need. But it also creates an opportunity to learn a little bit more about APIs than I knew before by building one from scratch. And that also means we'll need some data of our own. Once we have that, we can build our client-side processer and push the results to BigQuery. So here's what our steps will look like:

1. Set up two VMs to mimic having a client and server  
2. Download the flows.txt.gz file from [LANL](https://csr.lanl.gov/data/cyber1/)  
3. Clean the data  
4. Import the data into a sqlite database  
5. Create the API using Flask  
6. Grab a subset of the data  
7. Process it  
8. Send it to GBQ  
9. Set up a script to iteratively grab newer chunks of the data  
10. Profit

#### Set up our VMs
First and foremost, I want to say that I'm using two VMs because I didn't want to clutter my host with the stuff we'd be creating on the client machine. It's worth pointing out that you could absolutely create a single VM to serve as the server, and connect to the API from your host. Additionally, I'm not going to go into any level of detail here as there are TONS of blogs, videos, etc on how to set up a virtual machine.

But what I will say is that both machines are running Ubuntu 20.04 via VirtualBox using a Bridged network adapter. I'm not an expert, so it might be possible to achieve this using a NAT network adapter, but in order to get the two machines to be able to talk to each other I had to use Bridged. In order to ensure that your client VM can talk to the server VM, you should run `ip addr show` on both machines, and then have each machine ping the other, just to make sure they can see each other. 

For specs, I've set it up so that the VM acting as the server, which will house the sqlite database and the API, is using a single CPU and 2 GB of RAM. The VM that I'm using to actually pull the data from the API is using 2 CPU and 4 GB of RAM. Each machine has a cap of 100GB of storage. The purpose for doing this was to show that this method of handling the data as it streams in allows us to work with very large data using minimal hardware.

#### Download and clean the data
Getting the data is a bit of a hassle because LANL changed their website to now ask you for your email and what you're going to use the data for. You don't have to provide real answers, but I haven't seen them send any spam email to me, and seeing that people are using the data they provide likely incentives them to continue to provide data. So I'd encourage you to at least provide a real reason (e.g. "Creating an API from scratch and using this as the database"). Anyway, once you do that, you can just click the file and the download will start. I opted for the flows.txt.gz file because it was pretty small and we don't need a lot of data. The original ask involved about 50GB of data, that included all of the overhead of XML, and as we'll see later on, that small file ends up being a lot of data once we add in that overhead.

Once we have the data, we'll need to decompress it and then do a couple of preprocessing things. Running `gunzip flows.txt.gz` will give us the files that we need, so now we can begin preprocessing the data. The first thing we want to do is ensure that every row has only the data that we want. What this means is that if a data ingestion tool were to try to import this file, which is a CSV, using commas as delimiters, would it find the exact number of columns that it expected on each row? We can ensure that it does by check the number of columns ourself first. Since this is just test data, we don't have to worry about what to do if some of the records are incorrect, but in the real world that's definitely something you'd want to consider. For us, we can run the following command:

```
cat flows.txt | awk -F , 'NF == 9{print}{}' >> flows.csv
```

What this is doing is piping the file into awk and telling awk to use the comma as a delimeter (`-F ,`) and then check if there are 9 fields (`NF == 9`) on that line. If there are, then print the line (`{print}`), otherwise do nothing (`{}`). The output is then piped into flows.csv. The end result is a CSV file that is guaranteed to have the exact number of fields expected on each line, which will prevent any ingest errors when we load it into sqlite.

The next step is add the row number to each line. The reason for this is that the flows data does not have a single column that is unique to every row, and Flask will complain if you try to feed it a table that doesn't have a primary key defined in the table. This is kind of annoying because sqlite would have given us a primary key for free via `ROWID`, but Flask forces us to set our own. To do this, we'll again use awk.

```
mv flows.csv flows.txt
cat flows.txt | awk '{printf "%s,%s\n", NR, $0}' >> flows.csv
```

Again, we're piping the file into awk - and yes, I know you can just feed the file in; get off my back - and then using `printf` to format the line before printing. The `"%s,%s\n"` is the format of the line, which accepts two variables, designated by the `%s`, separated by a comma and followed by a new line. `NR` is the row number, and is our first variable, and `$0` is the line of data, and is our second variable. Once this is finished running, if you run `head -n 3 flows.csv`, you should see something that looks like this:

```
1,1,0,C1065,389,C3799,N104561,6,10,5323
2,1,0,C1423,N1136,C1707,N1,6,5,847
3,1,0,C1423,N1142,C1707,N1,6,5,847
```

#### Import the data into sqlite
Assuming that went well, we're ready to import the data into sqlite. To do this, we'll first need to install sqlite3 on our machine by running `sudo apt install sqlite3`. Once that's installed, run `sqlite3 flows.db` and that'll drop you into sqlite inside of the newly created flows.db file (it creates the file in the current working directory if it doesn't already exist). Your terminal should look something like this:

```
SQLite Version 3.31.1 2020-01-27 19:55:54
Enter ".help" for usage hints.
sqlite> 
```

Our next step is to create the table that we're going to store our data in. If you've worked with SQL before, this is probably pretty familiar to you, but if you haven't, I'd recommend taking some time to read a couple blog posts about it. But even if you haven't, all of the following will hopefully still be pretty readable for you. At the prompt, type the following snippet of code.

```
CREATE TABLE flows(
    id INTEGER PRIMARY KEY, 
    time INTEGER NOT NULL, 
    duration INTEGER NOT NULL, 
    src_comp TEXT NOT NULL, 
    src_port TEXT NOT NULL, 
    dst_comp TEXT NOT NULL, 
    dst_port TEXT NOT NULL, 
    protocol INTEGER NOT NULL, 
    packet_count INTEGER NOT NULL, 
    byte_count NOT NULL)
    WITHOUT ROWID;
```

If you don't get any errors, you should be able to run `.tables` and have it return "flows" to you. You should also be able to run `.schema` and have it return back what you just typed in. If that all works, we can now import our CSV that we created. One thing you may have noticed is that our CSV doesn't have a header row on it, saying what the columns are. This is by design since sqlite already knows what the columns are. It expects that the data being read in is in the order in which you specified the columns in your `CREATE TABLE` statment. So to do this, we'll run two commands. The first tells sqlite that we're about to deal with some CSV data, and the second imports the data.

```
.mode csv
.import /absolute/path/to/flows.csv flows
```

The first argument to `.import` is the path to the file you want to import, and the second argument is the name of the table you want to store it in. This is going to take a minute to run, so feel free to grab some coffee or something.

#### Create the API using Flask
Finally, we get to the fun part! Again, this is not a tutorial on all things, so I won't be diving into the granular details of Flask, but I'll be explaining the key things as we go. To get started, we need to install Flask on to our server VM (this is the one with 1 CPU and 2GB of RAM). To do this, we'll first need to install pip, so run the following commands on the VM that will be your server.

```
sudo app install python3-pip
pip3 install flask
```

To make sure this works, enter Python from the terminal and run `from flask import Flask`. If that works, you're good to go. So let's dive into the code. Create a file called "app.py". That's going to hold all of the code we need for our API. We'll start by just enumerating the modules we're importing.

```
from flask import Flask, request, Response
import sqlite3
import os
import json
```

This should be relatively straight-forward. The next thing we're going to do is initialize a couple of variables upfront.

```
basedir = os.path.abspath(os.path.dirname(__file__))
app = Flask(__name__)
```

The first line isn't really necessary, but it allows us to ensure that our base directory is always the directy in which app.py lives, which is useful for connecting to the database, and allows us to call app.py from anywhere. The second line initializes our Flask app using the `__name__` variable, which in our situation will just be equal to "__main__". You can read more about Flask [here](https://flask.palletsprojects.com/en/1.1x/api/). We'll finish with the skeleton of the file (the base code needed to run the Flask app) by telling the script to run the app if app.py is called directly.

```
if __name__ == "__main__":
    app.run(debug=True, host="0.0.0.0")
```

Setting debug equal to True allows us to keep the app running while we change things in app.py without having to stop the app and restart it. Setting host equal to "0.0.0.0" allows the app to listen for connections from more than just the localhost, which is crucial for being able to connect to the API from our client VM. 

Now that we have our skeleton, let's add some functionality so we can check that it's actually working. Add the following code just above the `if __name__ ==` line.

```
@app.route("/api/v1/stream/flows.json", methods=["GET"])
def get():
    return Response(
        json.dumps({"msg": "Hello World!"}), 
        content_type="application/json"
    )
```

Save the file and exit and then run `python3 app.py`. You should see something like the following:

```
* Serving Flask app "app" (lazy loading)
* Environment: production
  Warning: This is a development server. Do not use it in a production deployment.
  Use a production WSGI server instead.
* Debug mode: on
* Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)
* Restarting with stat
* Debugger is active!
* Debugger PIN: ###-###-###
```

If you see that, head over to your client VM and open a browser (or just use the browser on your host) and got to http://0.0.0.0:5000/api/v1/stream/flows.json. You should be created with JSON data that says "Hello world". But that's not a very useful API, so we'll add the real functionality now. The code we just added is what's called a route, and it tells Flask two things:

1. What end points should users be able to hit, and  
2. What Flask should do when users hit a specific end point.

So as you may have deduced from the code and the URL that you navigated to, "/api/v1/stream/flows.json" is the initial endpoint that we're creating for this API. The second parameter that was passed to `@app.route` is the methods that the end point should accept, with GET and POST being the two most common. The rest of the code that we added is what the endpoint should do when a user hits that endpoint with the method we specified. In our quick example, it returned a simple JSON-like string (that's what `json.dumps` does) to the client. We're going to change this code to now connect to our database, run a query, and stream the results back to the client. 

In your file, replace this:

```
def get():
    return Response(
        json.dumps({"msg": "Hello World!"}), 
        content_type="application/json"
    )
```

with this:

```
def get():
  conn = sqlite3.connect(basedir + "/flows.db")
  c = conn.cursor()
  q = "SELECT * FROM flows LIMIT 10"
  c.execute(q)
  return Response(stream(conn, c), content_type="application/json")
```

The first line of our new code is us connecting to our sqlite database. This is a persisten connection and will stay open until we close it, which you should be mindful of because too many open connections on a database is problematic. The second line creates a cursor inside of our database, which acts as a pointer. The third line is our query. We're going to grab all of the variables, but only the first 10 records for now. Best to test small before going big. The fourth line "executes" the query. I put execute in quotes because while it does execute the query, it doesn't actually return any of the data, which is advantageous for us. The last line returns the output of a currently undefined function called stream as a `Reponse` object. To really take advantage of this, we need to create a generator that will yield each record from our database to `Repsonse` so it can send it to the client and get ready for the next record. To do this, we'll create a new function called stream. Put the following code above your `@app.route` line.

```
def stream(conn, records):
  try:
    prev_record = next(records)
  except StopIteration:
    yield '{}'
    raise StopIteration
  
  for record in records:
    yield json.dumps(format_record(prev_record)) + "\n"
    prev_record = record
  
  conn.close()
  yield json.dumps(format_record(prev_record))
```

What we've built here is a lagged generator. Because we're appending a delimiter to each JSON record - here we're using a new line character - we need to make sure we don't append it to the last record that gets returned. Otherwise, we'd be sending malformed JSON to the user. To overcome this, we'll jump start the iteration by calling `next` on the iterator to get the first result before we even start our loop. What this means is that once we begin our loop via `for record in records`, the first value that `record` gets is actually the second value in `records`, not the first. That value is stored in `prev_record`. So we jump in at the second record, yield the first record, and then set `prev_record` to the current record. Assume there are five records in the table that we query. If you follow this through, you'll see that once we get to the fifth and final record, we'll be returning the fourth record with our delimiter attached, which means we still need to return our fifth record. We do that with our final `yield` call in the function, just after we close our connection to the database. If you're not familiar with the `yield` keyword, I would highly encourage looking it up, as it has some very powerful properties.

But let's look at this function with a little more detail and walk through it line by line. The first thing we notice is that the function takes two parameters: conn and records. If we look at the line in our code where we call stream, we'll see that we're conn, which is our database connection, as the first parameter, and c, which is our cursor that is holding the query we executed, as the second parameter. I named it records in the stream function because that's what it represents. `c` is an iterable that on each call will return the next record from the result set that is generated by the query. Once all results have been returned, `c` has been exhausted. So looping over `c` is really looping over the records. The next thing we do is utilize a try/except structure to capture any events in which our query returns no results. In this case, we'll return an empty JSON-like string. Assuming there are results, we loop over them returning the data as I described above until we've exhausted the results. Once all but the last record has been returned to the client, we close the database connection and then return that final record.

With this functionality in place, we're left with just one more function to define, which is the `format_record` function that we utilize in `stream`. This function does nothing other than convert the record that is returned from a tuple to a dict so that it can be passed as JSON to the client. It's defined below and should be self explantory.

```
def format_record(record):
  return {
    "id": record[0],
    "time": record[1],
    "duration": record[2],
    "src_comp": record[3],
    "src_port": record[4],
    "dst_comp": record[5],
    "dst_port": record[6],
    "protocol": record[7],
    "packet_count": record[8],
    "byte_count": record[9]
  }
```

And now, with that, you should be refresh the page in your browser that's calling the API and see the first 10 records of the flows table returned to you as JSON. Congratulations! You just built an API. 
    